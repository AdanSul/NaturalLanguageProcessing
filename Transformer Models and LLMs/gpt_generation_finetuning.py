import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_from_disk
from datasets import load_dataset
import os
import sys



SEED = 42
torch.manual_seed(SEED)
os.environ["WANDB_DISABLED"] = "true"

def get_imdb_subset(path):
    
    try:
        subset = load_from_disk(path)
    except Exception:
        dataset = load_dataset("imdb")
        subset = dataset["train"]

    positive_reviews = subset.filter(lambda x: x["label"] == 1).shuffle(seed=SEED).select(range(100))
    negative_reviews = subset.filter(lambda x: x["label"] == 0).shuffle(seed=SEED).select(range(100))

    return positive_reviews, negative_reviews

def tokenize_reviews(dataset, tokenizer, max_length=150):
    """
    מבצע טוקניזציה על הביקורות ומוסיף Padding.
    """
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            padding="max_length",
            truncation=True,
            max_length=max_length
        )

    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
    return tokenized_dataset

def train_model(model, tokenizer, dataset, output_dir):
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir='./logs',
        save_strategy="epoch",
        learning_rate=2e-5,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    )

    trainer.train()
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    return trainer.model

def generate_reviews(model, tokenizer, prompt, num_reviews=5,
                     max_length=200, temperature=0.7, top_k=50, top_p=0.9,
                     repetition_penalty=1.2):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    reviews = []


    for _ in range(num_reviews):
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
        attention_mask = input_ids.ne(tokenizer.pad_token_id).to(device)
        with torch.no_grad():
            output = model.generate(
                input_ids,
                attention_mask = attention_mask,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                do_sample=True,
                num_return_sequences=1
            )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        reviews.append(generated_text)

    return reviews


def save_reviews_to_file(positive_reviews, negative_reviews, output_file):
    try:
        with open(output_file, "w", encoding="utf-8") as file:
            file.write("Reviews generated by positive model:\n")  
            for i, review in enumerate(positive_reviews, 1): 
                file.write(f"{i}. {review}\n")

            file.write("\nReviews generated by negative model:\n")  
            for i, review in enumerate(negative_reviews, 1):  
                file.write(f"{i}. {review}\n")

    except Exception as e:
        pass


def main():
    
    if len(sys.argv) != 4:
        raise ValueError("Usage: python script.py <path_to_imdb_subset> <path_to_output_reviews.txt> <path_to_models>")
        sys.exit(1)

    subset_path = sys.argv[1]
    output_file = sys.argv[2]
    models_dir = sys.argv[3]

    os.makedirs(models_dir, exist_ok=True)

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    positive_model_path = os.path.join(models_dir, "positive_model")
    negative_model_path = os.path.join(models_dir, "negative_model")

    
    if os.path.exists(positive_model_path) and os.path.exists(negative_model_path):
        

        try:
            positive_model = GPT2LMHeadModel.from_pretrained(positive_model_path)
            negative_model = GPT2LMHeadModel.from_pretrained(negative_model_path)
        except Exception:
            positive_model, negative_model = None, None
    else:
        positive_model, negative_model = None, None

   
    if positive_model is None or negative_model is None:
        
        positive_reviews, negative_reviews = get_imdb_subset(subset_path)

        
        positive_model = GPT2LMHeadModel.from_pretrained("gpt2")
        tokenized_positive = tokenize_reviews(positive_reviews, tokenizer)
        positive_model = train_model(
            positive_model,
            tokenizer,
            tokenized_positive,
            positive_model_path
        )
       
        negative_model = GPT2LMHeadModel.from_pretrained("gpt2")
        tokenized_negative = tokenize_reviews(negative_reviews, tokenizer)
        negative_model = train_model(
            negative_model,
            tokenizer,
            tokenized_negative,
            negative_model_path
        )
        
    prompt = "The movie was"
   
    generated_positive = generate_reviews(positive_model, tokenizer, prompt)

    
    generated_negative = generate_reviews(negative_model, tokenizer, prompt)

    
    save_reviews_to_file(generated_positive, generated_negative, output_file)
    

if __name__ == "__main__":
    main()
