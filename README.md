# Natural Language Processing

Welcome to this repository dedicated to Natural Language Processing (NLP). Here, you will find projects exploring diverse NLP techniques, understanding their applications, and implementing them from scratch.

## Table of Contents
- [Introduction](#introduction)
- [Projects](#Projects)
  - [Cleaning and Processing the Corpus](#Cleaning-and-Processing-the-Corpus)
  - [Building Language Models and Collections](#Building-Language-Models-and-Collections)
  - [Text Classification](#Text-Classification)
  - [Word Embeddings](#Word-Embeddings)
  - [Transformer Models and LLMs](#Transformer-Models-and-LLMs)
  

## Introduction

Natural Language Processing (NLP) bridges the gap between human language and computer understanding. It enables computers to analyze, interpret, and respond to textual data in meaningful ways.

This repository showcases hands-on projects in NLP, focusing on:

- Statistical models for language processing,
- Classification tasks for textual data,
- Building models from scratch,
- Representation and comparison of sentences using advanced techniques.


  
## Projects

### Cleaning and Processing the Corpus

Developed methods to preprocess the corpus and extract key information, such as identifying names and capturing dialogue.


### Building Language Models and Collections

Developed statistical language models using n-grams to analyze word predictions. Created collections to process sentence structures and extract meaningful patterns from the corpus.


### Text Classification

Developed a classification system to address linguistic challenges using machine learning models. Implemented a pipeline for training and evaluating classifiers on structured text data extracted from protocols, leveraging the scikit-learn library for advanced preprocessing and model training.


### Word Embeddings

Trained a Word2Vec model on Knesset protocols to generate word vectors capturing semantic relationships. Evaluated word and sentence similarities using cosine similarity and explored contextual word replacements. Applied embeddings for sentence representation and classification tasks.


### Transformer Models and LLMs

Fine-tuned BERT for sentiment classification on IMDB reviews. Trained GPT-2 to generate positive and negative reviews. Used FLAN-T5 for prompt-based classification, exploring zero-shot, few-shot, and instruction-based prompting. Analyzed bias in LLMs.
